{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 計算グラフを設計"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNの計算グラフ\n",
    "URL: http://www.ie110704.net/2017/10/24/tensorflow%E3%81%A7%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E3%80%81cnn%E3%82%92%E5%AE%9F%E8%A3%85%E3%81%97%E3%81%A6%E3%81%BF%E3%81%9F/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow API: https://qiita.com/piyo_papa/items/1fca6138645a29a31245"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算グラフ\n",
    "\n",
    "# プレースホルダー\n",
    "x_ = tf.placeholder(tf.float32, shape=(None, 28, 28, 1))\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, 10))\n",
    "\n",
    "# 畳み込み層1\n",
    "with tf.name_scope('conv1'):\n",
    "    conv1_features = 20 # 畳み込み層1の出力次元数\n",
    "    max_pool_size1 = 2 # 畳み込み層1のマックスプーリングサイズ\n",
    "    conv1_w = tf.Variable(tf.truncated_normal([5, 5, 1, conv1_features], stddev=0.1), dtype=tf.float32, name='conv1_w') # 畳み込み層1の重み(初期値)\n",
    "    conv1_b = tf.Variable(tf.constant(0.1, shape=[conv1_features]), dtype=tf.float32, name='conv1_b') # 畳み込み層1のバイアス(初期値)\n",
    "    conv1_c2 = tf.nn.conv2d(x_, conv1_w, strides=[1, 1, 1, 1], padding=\"SAME\", name='conv1_conv2d') # 畳み込み層1-畳み込み\n",
    "    conv1_relu = tf.nn.relu(conv1_c2+conv1_b, name='conv1_ReLU') # 畳み込み層1-ReLU\n",
    "    conv1_mp = tf.nn.max_pool(conv1_relu, ksize=[1, max_pool_size1, max_pool_size1, 1], strides=[1, max_pool_size1, max_pool_size1, 1], padding=\"SAME\", name='conv1_max_polling') #畳み込み層1-マックスプーリング\n",
    "\n",
    "# 畳み込み層2\n",
    "with tf.name_scope('conv2'):\n",
    "    conv2_features = 50 # 畳み込み層2の出力次元数\n",
    "    max_pool_size2 = 2 # 畳み込み層2のマックスプーリングのサイズ\n",
    "    conv2_w = tf.Variable(tf.truncated_normal([5, 5, conv1_features, conv2_features], stddev=0.1), dtype=tf.float32, name='conv2_w') # 畳み込み層2の重み\n",
    "    conv2_b = tf.Variable(tf.constant(0.1, shape=[conv2_features]), dtype=tf.float32, name='conv2_b') # 畳み込み層2のバイアス\n",
    "    conv2_c2 = tf.nn.conv2d(conv1_mp, conv2_w, strides=[1, 1, 1, 1], padding=\"SAME\", name='conv2_conv2d') # 畳み込み層2-畳み込み\n",
    "    conv2_relu = tf.nn.relu(conv2_c2+conv2_b, name='conv2_ReLU') # 畳み込み層2-ReLU\n",
    "    conv2_mp = tf.nn.max_pool(conv2_relu, ksize=[1, max_pool_size2, max_pool_size2, 1], strides=[1, max_pool_size2, max_pool_size2, 1], padding=\"SAME\", name='conv2_max_polling') # 畳み込み層2-マックスプーリング\n",
    "\n",
    "# 全結合層1\n",
    "with tf.name_scope('fully1'):\n",
    "    result_w = x_.shape[1] // (max_pool_size1*max_pool_size2)\n",
    "    result_h = x_.shape[2] // (max_pool_size1*max_pool_size2)\n",
    "    fc_input_size = result_w * result_h * conv2_features # 畳み込んだ結果、全結合層に入力する次元数\n",
    "    fc_features = 500 # 全結合層の出力次元数（隠れ層の次元数）\n",
    "    s = conv2_mp.get_shape().as_list() # [None, result_w, result_h, conv2_features]\n",
    "    conv_result = tf.reshape(conv2_mp, [-1, s[1]*s[2]*s[3]]) # 畳み込みの結果を1*N層に変換\n",
    "    fc1_w = tf.Variable(tf.truncated_normal([fc_input_size.value, fc_features], stddev=0.1), dtype=tf.float32, name='fully1_w') # 重み\n",
    "    fc1_b = tf.Variable(tf.constant(0.1, shape=[fc_features]), dtype=tf.float32, name='fully1_b') # バイアス\n",
    "    fc1 = tf.nn.relu(tf.matmul(conv_result, fc1_w)+fc1_b, name='fully1_ReLU') # 全結合層1\n",
    "\n",
    "# 全結合層2\n",
    "with tf.name_scope('fully2'):\n",
    "    fc2_w = tf.Variable(tf.truncated_normal([fc_features, fc_features], stddev=0.1), dtype=tf.float32, name='fully2_w') # 重み\n",
    "    fc2_b = tf.Variable(tf.constant(0.1, shape=[fc_features]), dtype=tf.float32, name='fully2_b') # バイアス\n",
    "    fc2 = tf.nn.relu(tf.matmul(fc1, fc2_w)+fc2_b, name='fully1_ReLU') # 全結合層2\n",
    "\n",
    "# 全結合層3\n",
    "with tf.name_scope('fully3'):\n",
    "    fc3_w = tf.Variable(tf.truncated_normal([fc_features, 10], stddev=0.1), dtype=tf.float32, name='fully3_w') # 重み\n",
    "    fc3_b = tf.Variable(tf.constant(0.1, shape=[10]), dtype=tf.float32, name='fully3_b') # バイアス\n",
    "    y = tf.matmul(fc2, fc3_w)+fc3_b\n",
    "\n",
    "# クロスエントロピー誤差(改訂版)\n",
    "with tf.name_scope('loss'):\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=y))\n",
    "# 勾配法\n",
    "with tf.name_scope('train'):\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "\n",
    "# TensorBoardで追跡する変数を定義\n",
    "file_writer = tf.summary.FileWriter(\"logsdata/CNN_graph\", tf.get_default_graph())\n",
    "file_writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inverterの計算グラフ設計"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(?, 128, 128, 3), dtype=float32)\n",
      "Tensor(\"Placeholder_1:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"Placeholder_2:0\", dtype=float32)\n",
      "Tensor(\"fully4/add:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"loss/loss:0\", shape=(), dtype=float32)\n",
      "name: \"train/Adam\"\n",
      "op: \"NoOp\"\n",
      "input: \"^train/Adam/update_conv1/conv1_w/ApplyAdam\"\n",
      "input: \"^train/Adam/update_conv1/conv1_b/ApplyAdam\"\n",
      "input: \"^train/Adam/update_conv2/conv2_w/ApplyAdam\"\n",
      "input: \"^train/Adam/update_conv2/conv2_b/ApplyAdam\"\n",
      "input: \"^train/Adam/update_conv3/conv3_w/ApplyAdam\"\n",
      "input: \"^train/Adam/update_conv3/conv3_b/ApplyAdam\"\n",
      "input: \"^train/Adam/update_conv4/conv4_w/ApplyAdam\"\n",
      "input: \"^train/Adam/update_conv4/conv4_b/ApplyAdam\"\n",
      "input: \"^train/Adam/update_conv5/conv5_w/ApplyAdam\"\n",
      "input: \"^train/Adam/update_conv5/conv5_b/ApplyAdam\"\n",
      "input: \"^train/Adam/update_conv6/conv6_w/ApplyAdam\"\n",
      "input: \"^train/Adam/update_conv6/conv6_b/ApplyAdam\"\n",
      "input: \"^train/Adam/update_fully1/fully1_w/ApplyAdam\"\n",
      "input: \"^train/Adam/update_fully1/fully1_b/ApplyAdam\"\n",
      "input: \"^train/Adam/update_fully2/fully2_w/ApplyAdam\"\n",
      "input: \"^train/Adam/update_fully2/fully2_b/ApplyAdam\"\n",
      "input: \"^train/Adam/update_fully3/fully3_w/ApplyAdam\"\n",
      "input: \"^train/Adam/update_fully3/fully3_b/ApplyAdam\"\n",
      "input: \"^train/Adam/update_fully4/fully4_w/ApplyAdam\"\n",
      "input: \"^train/Adam/update_fully4/fully4_b/ApplyAdam\"\n",
      "input: \"^train/Adam/Assign\"\n",
      "input: \"^train/Adam/Assign_1\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# 計算グラフ\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    # プレースホルダー\n",
    "    x_ = tf.placeholder(tf.float32, shape=(None, 128, 128, 3))\n",
    "    y_ = tf.placeholder(tf.float32, shape=(None, 100))\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    print(x_)\n",
    "    print(y_)\n",
    "    print(keep_prob)\n",
    "    \n",
    "    # 畳み込み層1\n",
    "    with tf.name_scope('conv1'):\n",
    "        conv1_features = 40 # 畳み込み層1の出力次元数\n",
    "        max_pool_size1 = 2 # 畳み込み層1のマックスプーリングサイズ\n",
    "        conv1_w = tf.Variable(tf.truncated_normal([5, 5, 3, conv1_features], stddev=0.1), dtype=tf.float32, name='conv1_w') # 畳み込み層1の重み(初期値)\n",
    "        conv1_b = tf.Variable(tf.constant(0.1, shape=[conv1_features]), dtype=tf.float32, name='conv1_b') # 畳み込み層1のバイアス(初期値)\n",
    "        conv1_c2 = tf.nn.conv2d(x_, conv1_w, strides=[1, 1, 1, 1], padding=\"SAME\", name='conv1_conv2d') # 畳み込み層1-畳み込み\n",
    "        conv1_relu = tf.nn.relu(conv1_c2+conv1_b, name='conv1_ReLU') #畳み込み層1-ReLU\n",
    "        conv1_drop = tf.nn.dropout(conv1_relu, keep_prob, name='conv1_dropout')#畳み込み層1-ドロップアウト\n",
    "        conv1_mp = tf.nn.max_pool(conv1_drop, ksize=[1, max_pool_size1, max_pool_size1, 1], strides=[1, max_pool_size1, max_pool_size1, 1], padding=\"SAME\", name='conv1_max_polling') #畳み込み層1-マックスプーリング\n",
    "\n",
    "    # 畳み込み層2\n",
    "    with tf.name_scope('conv2'):\n",
    "        conv2_features = 80 # 畳み込み層2の出力次元数\n",
    "        max_pool_size2 = 2 # 畳み込み層2のマックスプーリングのサイズ\n",
    "        conv2_w = tf.Variable(tf.truncated_normal([5, 5, conv1_features, conv2_features], stddev=0.1), dtype=tf.float32, name='conv2_w') # 畳み込み層2の重み\n",
    "        conv2_b = tf.Variable(tf.constant(0.1, shape=[conv2_features]), dtype=tf.float32, name='conv2_b') # 畳み込み層2のバイアス\n",
    "        conv2_c2 = tf.nn.conv2d(conv1_mp, conv2_w, strides=[1, 1, 1, 1], padding=\"SAME\", name='conv2_conv2d') # 畳み込み層2-畳み込み\n",
    "        conv2_relu = tf.nn.relu(conv2_c2+conv2_b, name='conv2_ReLU') # 畳み込み層2-ReLU\n",
    "        conv2_drop = tf.nn.dropout(conv2_relu, keep_prob, name='conv2_dropout')#畳み込み層2-ドロップアウト\n",
    "        conv2_mp = tf.nn.max_pool(conv2_drop, ksize=[1, max_pool_size2, max_pool_size2, 1], strides=[1, max_pool_size2, max_pool_size2, 1], padding=\"SAME\", name='conv2_max_polling') # 畳み込み層2-マックスプーリング\n",
    "\n",
    "    # 畳み込み層3\n",
    "    with tf.name_scope('conv3'):\n",
    "        conv3_features = 160 # 畳み込み層3の出力次元数\n",
    "        max_pool_size3 = 2 # 畳み込み層3のマックスプーリングのサイズ\n",
    "        conv3_w = tf.Variable(tf.truncated_normal([5, 5, conv2_features, conv3_features], stddev=0.1), dtype=tf.float32, name='conv3_w') # 畳み込み層3の重み\n",
    "        conv3_b = tf.Variable(tf.constant(0.1, shape=[conv3_features]), dtype=tf.float32, name='conv3_b') # 畳み込み層3のバイアス\n",
    "        conv3_c2 = tf.nn.conv2d(conv2_mp, conv3_w, strides=[1, 1, 1, 1], padding=\"SAME\", name='conv3_conv2d') # 畳み込み層3-畳み込み\n",
    "        conv3_relu = tf.nn.relu(conv3_c2+conv3_b, name='conv3_ReLU') # 畳み込み層3-ReLU\n",
    "        conv3_drop = tf.nn.dropout(conv3_relu, keep_prob, name='conv3_dropout')#畳み込み層3-ドロップアウト\n",
    "        conv3_mp = tf.nn.max_pool(conv3_drop, ksize=[1, max_pool_size3, max_pool_size3, 1], strides=[1, max_pool_size3, max_pool_size3, 1], padding=\"SAME\", name='conv3_max_polling') # 畳み込み層3-マックスプーリング\n",
    "    \n",
    "    # 畳み込み層4\n",
    "    with tf.name_scope('conv4'):\n",
    "        conv4_features = 320 # 畳み込み層4の出力次元数\n",
    "        max_pool_size4 = 2 # 畳み込み層4のマックスプーリングのサイズ\n",
    "        conv4_w = tf.Variable(tf.truncated_normal([5, 5, conv3_features, conv4_features], stddev=0.1), dtype=tf.float32, name='conv4_w') # 畳み込み層4の重み\n",
    "        conv4_b = tf.Variable(tf.constant(0.1, shape=[conv4_features]), dtype=tf.float32, name='conv4_b') # 畳み込み層4のバイアス\n",
    "        conv4_c2 = tf.nn.conv2d(conv3_mp, conv4_w, strides=[1, 1, 1, 1], padding=\"SAME\", name='conv4_conv2d') # 畳み込み層4-畳み込み\n",
    "        conv4_relu = tf.nn.relu(conv4_c2+conv4_b, name='conv4_ReLU') # 畳み込み層4-ReLU\n",
    "        conv4_drop = tf.nn.dropout(conv4_relu, keep_prob, name='conv4_dropout')#畳み込み層4-ドロップアウト\n",
    "        conv4_mp = tf.nn.max_pool(conv4_drop, ksize=[1, max_pool_size4, max_pool_size4, 1], strides=[1, max_pool_size4, max_pool_size4, 1], padding=\"SAME\", name='conv4_max_polling') # 畳み込み層4-マックスプーリング\n",
    "\n",
    "    # 畳み込み層5\n",
    "    with tf.name_scope('conv5'):\n",
    "        conv5_features = 640 # 畳み込み層5の出力次元数\n",
    "        max_pool_size5 = 2 # 畳み込み層5のマックスプーリングのサイズ\n",
    "        conv5_w = tf.Variable(tf.truncated_normal([5, 5, conv4_features, conv5_features], stddev=0.1), dtype=tf.float32, name='conv5_w') # 畳み込み層5の重み\n",
    "        conv5_b = tf.Variable(tf.constant(0.1, shape=[conv5_features]), dtype=tf.float32, name='conv5_b') # 畳み込み層5のバイアス\n",
    "        conv5_c2 = tf.nn.conv2d(conv4_mp, conv5_w, strides=[1, 1, 1, 1], padding=\"SAME\", name='conv5_conv2d') # 畳み込み層5-畳み込み\n",
    "        conv5_relu = tf.nn.relu(conv5_c2+conv5_b, name='conv5_ReLU') # 畳み込み層5-ReLU\n",
    "        conv5_drop = tf.nn.dropout(conv5_relu, keep_prob, name='conv5_dropout')#畳み込み層5-ドロップアウト\n",
    "        conv5_mp = tf.nn.max_pool(conv5_drop, ksize=[1, max_pool_size5, max_pool_size5, 1], strides=[1, max_pool_size5, max_pool_size5, 1], padding=\"SAME\", name='conv5_max_polling') # 畳み込み層5-マックスプーリング\n",
    "\n",
    "    # 畳み込み層6\n",
    "    with tf.name_scope('conv6'):\n",
    "        conv6_features = 1280 # 畳み込み層6の出力次元数\n",
    "        max_pool_size6 = 2 # 畳み込み層6のマックスプーリングのサイズ\n",
    "        conv6_w = tf.Variable(tf.truncated_normal([5, 5, conv5_features, conv6_features], stddev=0.1), dtype=tf.float32, name='conv6_w') # 畳み込み層6の重み\n",
    "        conv6_b = tf.Variable(tf.constant(0.1, shape=[conv6_features]), dtype=tf.float32, name='conv6_b') # 畳み込み層6のバイアス\n",
    "        conv6_c2 = tf.nn.conv2d(conv5_mp, conv6_w, strides=[1, 1, 1, 1], padding=\"SAME\", name='conv6_conv2d') # 畳み込み層6-畳み込み\n",
    "        conv6_relu = tf.nn.relu(conv6_c2+conv6_b, name='conv6_ReLU') # 畳み込み層6-ReLU\n",
    "        conv6_drop = tf.nn.dropout(conv6_relu, keep_prob, name='conv6_dropout')#畳み込み層6-ドロップアウト\n",
    "        conv6_mp = tf.nn.max_pool(conv6_drop, ksize=[1, max_pool_size6, max_pool_size6, 1], strides=[1, max_pool_size6, max_pool_size6, 1], padding=\"SAME\", name='conv6_max_polling') # 畳み込み層6-マックスプーリング\n",
    "        \n",
    "    \n",
    "    # 全結合層1\n",
    "    with tf.name_scope('fully1'):\n",
    "        result_w = x_.shape[1] // (max_pool_size1*max_pool_size2*max_pool_size3*max_pool_size4*max_pool_size5*max_pool_size6)\n",
    "        result_h = x_.shape[2] // (max_pool_size1*max_pool_size2*max_pool_size3*max_pool_size4*max_pool_size5*max_pool_size6)\n",
    "        fc_input_size = result_w * result_h * conv6_features # 畳み込んだ結果、全結合層に入力する次元数\n",
    "        fc_features = 1000 # 全結合層1の出力次元数（隠れ層の次元数）\n",
    "        s = conv6_mp.get_shape().as_list() # [None, result_w, result_h, conv4_features]\n",
    "        conv_result = tf.reshape(conv6_mp, [-1, s[1]*s[2]*s[3]]) # 畳み込みの結果を1*N層に変換\n",
    "        fc1_w = tf.Variable(tf.truncated_normal([fc_input_size.value, fc_features], stddev=0.1), dtype=tf.float32, name='fully1_w') # 重み\n",
    "        fc1_b = tf.Variable(tf.constant(0.1, shape=[fc_features]), dtype=tf.float32, name='fully1_b') # バイアス\n",
    "        fc1 = tf.nn.relu(tf.matmul(conv_result, fc1_w)+fc1_b, name='fully1_ReLU') # 全結合層1\n",
    "        fc1_drop = tf.nn.dropout(fc1, keep_prob, name='fully1_dropout') #ドロップアウト\n",
    "    \n",
    "    # 全結合層2\n",
    "    with tf.name_scope('fully2'):\n",
    "        fc2_features = 1000 # 全結合層2の出力次元数（隠れ層の次元数）\n",
    "        fc2_w = tf.Variable(tf.truncated_normal([fc_features, fc2_features], stddev=0.1), dtype=tf.float32, name='fully2_w') # 重み\n",
    "        fc2_b = tf.Variable(tf.constant(0.1, shape=[fc2_features]), dtype=tf.float32, name='fully2_b') # バイアス\n",
    "        fc2 = tf.nn.relu(tf.matmul(fc1_drop, fc2_w)+fc2_b, name='fully2_ReLU') # 全結合層2\n",
    "        fc2_drop = tf.nn.dropout(fc2, keep_prob, name='fully2_dropout')#ドロップアウト\n",
    "\n",
    "    # 全結合層3\n",
    "    with tf.name_scope('fully3'):\n",
    "        fc3_features = 500 # 全結合層3の出力次元数（隠れ層の次元数）\n",
    "        fc3_w = tf.Variable(tf.truncated_normal([fc2_features, fc3_features], stddev=0.1), dtype=tf.float32, name='fully3_w') # 重み\n",
    "        fc3_b = tf.Variable(tf.constant(0.1, shape=[fc3_features]), dtype=tf.float32, name='fully3_b') # バイアス\n",
    "        fc3 = tf.nn.relu(tf.matmul(fc2_drop, fc3_w)+fc3_b, name='fully3_ReLU') # 全結合層3\n",
    "        fc3_drop = tf.nn.dropout(fc3, keep_prob, name='fully3_dropout')#ドロップアウト\n",
    "    \n",
    "    # 全結合層4\n",
    "    with tf.name_scope('fully4'):\n",
    "        fc4_features = 100 # 全結合層3の出力次元数（隠れ層の次元数）\n",
    "        fc4_w = tf.Variable(tf.truncated_normal([fc3_features, fc4_features], stddev=0.1), dtype=tf.float32, name='fully4_w') # 重み\n",
    "        fc4_b = tf.Variable(tf.constant(0.1, shape=[fc4_features]), dtype=tf.float32, name='fully4_b') # バイアス\n",
    "        y = tf.matmul(fc3_drop, fc4_w)+fc4_b\n",
    "        print(y)\n",
    "\n",
    "    # 平均2乗和誤差(square_error)\n",
    "    with tf.name_scope('loss'):\n",
    "        loss = tf.reduce_sum(tf.square(y_ - y), name='loss')\n",
    "        print(loss)\n",
    "    \n",
    "    # 勾配法\n",
    "    with tf.name_scope('train'):\n",
    "        train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "        print(train_step)\n",
    "\n",
    "    # グラフを保存する．\n",
    "    #saver = tf.train.Saver()\n",
    " \n",
    "    \"\"\"# TensorBoardで追跡する変数を定義\n",
    "    file_writer = tf.summary.FileWriter(\"logsdata/inverter_graph\", tf.get_default_graph())\n",
    "    file_writer.close()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データ読み込み&データ生成(バッチによる読み込み)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 128, 128, 3)\n",
      "(1, 100)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "i = 28901\n",
    "rec1 = np.load('/opt/pfw/dragon_ball_reconsted/reconsted/disc11/story1/img_'+str(i)+'_reconsted.npy')\n",
    "z1 = np.load('/opt/pfw/dragon_ball_reconsted/z/disc11/story1/img_'+str(i)+'_z.npy')\n",
    "print(rec1.shape)\n",
    "print(z1.shape)\n",
    "#Noneがあるので，4階テンソルでも大丈夫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22711\n",
      "22711\n",
      "(60, 128, 128, 3)\n",
      "(60, 128, 128, 3)\n",
      "(60, 100)\n",
      "(60, 100)\n",
      "4.973942518234253\n"
     ]
    }
   ],
   "source": [
    "#Noneがあるので，1つのファイルが4階テンソルでも大丈夫\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "batch_size = 60\n",
    "\n",
    "#再構成した画像のPATHのリスト\n",
    "path_rec = \"/opt/pfw/dragon_ball_reconsted/reconsted/disc11/story1/*.npy\"\n",
    "filelists_rec = glob.glob(path_rec, recursive=True)\n",
    "print(len(filelists_rec))\n",
    "\n",
    "#zのPATHのリスト\n",
    "path_z = \"/opt/pfw/dragon_ball_reconsted/z/disc11/story1/*.npy\"\n",
    "filelists_z = glob.glob(path_z, recursive=True)\n",
    "print(len(filelists_z))\n",
    "\n",
    "#サンプルしてくるインデックスのリスト\n",
    "ind = random.sample(range(len(filelists_rec)), batch_size)\n",
    "#print(\"index\\n%s\\n\" %ind)\n",
    "\n",
    "#サンプルしてくるインデックスの各PATHを格納したリスト\n",
    "batch_images_rec = [filelists_rec[i] for i in ind ]\n",
    "batch_images_z = [filelists_z[i] for i in ind ]\n",
    "\n",
    "#再構成した画像とzの各バッチ\n",
    "batch_x = np.array([np.load(f).tolist() for f in batch_images_rec]) #再構成画像のファイル読み込み\n",
    "batch_z = np.array([np.load(f).tolist() for f in batch_images_z]) #zのファイル読み込み\n",
    "\n",
    "print(np.array(batch_x).shape)\n",
    "print(np.array(batch_x).shape)\n",
    "\n",
    "print(np.array(batch_z).shape)\n",
    "print(np.array(batch_z).shape)\n",
    "\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データ読み込み&データ生成(バッチによる読み込み)の関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(batch_size):\n",
    "    #batch_size = 60\n",
    "\n",
    "    #再構成した画像のPATHのリスト\n",
    "    path_rec = \"/opt/pfw/dragon_ball_reconsted/reconsted/disc11/story1/*.npy\"\n",
    "    filelists_rec = glob.glob(path_rec, recursive=True)\n",
    "\n",
    "    #zのPATHのリスト\n",
    "    path_z = \"/opt/pfw/dragon_ball_reconsted/z/disc11/story1/*.npy\"\n",
    "    filelists_z = glob.glob(path_z, recursive=True)\n",
    "\n",
    "    #サンプルしてくるインデックスのリスト\n",
    "    ind = ra.sample(range(len(filelists_rec)), batch_size)\n",
    "\n",
    "    #サンプルしてくるインデックスの各PATHを格納したリスト\n",
    "    batch_images_rec = [filelists_rec[i] for i in ind ]\n",
    "    batch_images_z = [filelists_z[i] for i in ind ]\n",
    "\n",
    "    #再構成した画像とzの各バッチ\n",
    "    batch_x = np.array([np.load(f).tolist() for f in batch_images_rec]) #再構成画像のファイル読み込み\n",
    "    batch_z = np.array([np.load(f).tolist() for f in batch_images_z]) #zのファイル読み込み\n",
    "    \n",
    "    return batch_x, batch_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inverter実行フェーズ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"summary/Merge/MergeSummary:0\", shape=(), dtype=string)\n",
      "Epoch    0: 16134639500394496.0000 \t time   15\n",
      "Epoch    1: 9326594497708032.0000 \t time   61\n",
      "Epoch    2: 5788967374946304.0000 \t time   68\n",
      "Epoch    3: 3654511168061440.0000 \t time   75\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random as ra\n",
    "import os\n",
    "import glob\n",
    "from numpy.random import *\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "#実行フェーズ\n",
    "n_epochs = 50000000\n",
    "training_loss = []\n",
    "eps = 0.01\n",
    "batch_size = 60\n",
    "batch_x, batch_z = batch(batch_size)\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    \n",
    "    #変数を初期化\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # TensorBoardで追跡する変数を定義\n",
    "    with tf.name_scope('summary'):\n",
    "        tf.summary.scalar('loss', loss)\n",
    "        merged = tf.summary.merge_all()\n",
    "        writer = tf.summary.FileWriter('logsdata/inverter_graph_try', sess.graph)\n",
    "        print(merged)\n",
    "    \n",
    "    \n",
    "    #500エポックでモデルをトレーニング\n",
    "    for e in range(n_epochs):\n",
    "        c, _, summary = sess.run([loss, train_step, merged], feed_dict={x_: batch_x, y_: batch_z, keep_prob: 0.5})\n",
    "        writer.add_summary(summary, e)\n",
    "        training_loss.append(c)\n",
    "        \n",
    "        print('Epoch %4d: %.4f' % (e, c), '\\t', 'time %4d' %(time.time()-start))\n",
    "        \n",
    "        if not e % 50:\n",
    "            batch_x, batch_z = batch(batch_size)\n",
    "        \n",
    "        if(e > 0 and c < eps):\n",
    "            saver.save(sess, './savepoint/inverter_graph_try/train_epoch_'+str(e)+'/trained-model')\n",
    "            break\n",
    "        \n",
    "        if(e % 1000 == 0):\n",
    "            saver.save(sess, './savepoint/inverter_graph_try/train_epoch_'+str(e)+'/trained-model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習途中からstartする方法(途中から学習できるが，tensorboardに記録がつかない)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./savepoint/inverter_graph/train_epoch_23000/trained-model\n",
      "8037.0073\n",
      "None\n",
      "Epoch 23001: 8194.6553 \t time   23\n",
      "Epoch 23002: 8216.8291 \t time   32\n",
      "Epoch 23003: 8266.0254 \t time   40\n",
      "Epoch 23004: 8249.9365 \t time   51\n",
      "Epoch 23005: 8214.6309 \t time   60\n",
      "Epoch 23006: 8213.3750 \t time   67\n",
      "Epoch 23007: 8240.7812 \t time   76\n",
      "Epoch 23008: 8221.1699 \t time   84\n",
      "Epoch 23009: 8239.7012 \t time   93\n",
      "Epoch 23010: 8235.5342 \t time  102\n",
      "Epoch 23011: 8235.9404 \t time  111\n",
      "Epoch 23012: 8260.5195 \t time  121\n",
      "Epoch 23013: 8226.6592 \t time  131\n",
      "Epoch 23014: 8265.2041 \t time  143\n",
      "Epoch 23015: 8215.2227 \t time  153\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random as ra\n",
    "import os\n",
    "import glob\n",
    "from numpy.random import *\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "n_epochs = 50000000\n",
    "training_loss = []\n",
    "eps = 0.01\n",
    "batch_size = 60\n",
    "batch_x, batch_z = batch(batch_size)\n",
    "kp_p = 1.0\n",
    "\n",
    "epo = 23000\n",
    "\n",
    "g2 = tf.Graph()\n",
    "with tf.Session(graph=g2) as sess:\n",
    "    #計算グラフの復元\n",
    "    new_saver = tf.train.import_meta_graph('./savepoint/inverter_graph/train_epoch_'+str(epo)+'/trained-model.meta')\n",
    "    #実行フェーズでの値の復元\n",
    "    new_saver.restore(sess, './savepoint/inverter_graph/train_epoch_'+str(epo)+'/trained-model')\n",
    "    \n",
    "    loss = sess.run('loss/loss:0', feed_dict={'Placeholder:0' : batch_x, 'Placeholder_1:0' : batch_z, 'Placeholder_2:0' : kp_p})\n",
    "    train_step = sess.run('train/Adam', feed_dict={'Placeholder:0' : batch_x, 'Placeholder_1:0' : batch_z, 'Placeholder_2:0' : kp_p})\n",
    "    print(loss)\n",
    "    print(train_step)\n",
    "    \n",
    "    # グラフを保存する．\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # TensorBoardで追跡する変数を定義\n",
    "    with tf.name_scope('summary'):\n",
    "        tf.summary.scalar('loss', loss)\n",
    "        merged = tf.summary.merge_all()\n",
    "        writer = tf.summary.FileWriter('logsdata/inverter_graph_try2', sess.graph)\n",
    "    \n",
    "    #500エポックでモデルをトレーニング\n",
    "    for e in range(n_epochs):\n",
    "        e = epo+e+1\n",
    "        c, _, summary = sess.run(['loss/loss:0', 'train/Adam', merged], feed_dict={'Placeholder:0' : batch_x, 'Placeholder_1:0' : batch_z, 'Placeholder_2:0' : 0.5})\n",
    "        writer.add_summary(summary, e)\n",
    "        training_loss.append(c)\n",
    "        \n",
    "        print('Epoch %4d: %.4f' % (e, c), '\\t', 'time %4d' %(time.time()-start))\n",
    "        \n",
    "        if not e % 50:\n",
    "            batch_x, batch_z = batch(batch_size)\n",
    "        \n",
    "        if(e > 0 and c < eps):\n",
    "            saver.save(sess, './savepoint/inverter_graph_try2/train_epoch_'+str(e)+'/trained-model')\n",
    "            break\n",
    "        \n",
    "        if(e % 1000 == 0):\n",
    "            saver.save(sess, './savepoint/inverter_graph_try2/train_epoch_'+str(e)+'/trained-model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習途中からstartする方法(計算グラフを持ってきて，tensorboardに記録をつける．)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./savepoint/inverter_graph/train_epoch_23000/trained-model\n",
      "Epoch 23001: 8522.3438 \t time   13\n",
      "Epoch 23002: 8510.5488 \t time   20\n",
      "Epoch 23003: 8530.4004 \t time   27\n",
      "Epoch 23004: 8506.2061 \t time   34\n",
      "Epoch 23005: 8484.5254 \t time   41\n",
      "Epoch 23006: 8537.2529 \t time   48\n",
      "Epoch 23007: 8526.0908 \t time   55\n",
      "Epoch 23008: 8449.5430 \t time   61\n",
      "Epoch 23009: 8526.8652 \t time   68\n",
      "Epoch 23010: 8477.0176 \t time   75\n",
      "Epoch 23011: 8535.7549 \t time   82\n",
      "Epoch 23012: 8458.3369 \t time   89\n",
      "Epoch 23013: 8498.4834 \t time   96\n",
      "Epoch 23014: 8529.1543 \t time  103\n",
      "Epoch 23015: 8470.9932 \t time  110\n",
      "Epoch 23016: 8492.1816 \t time  117\n",
      "Epoch 23017: 8578.6582 \t time  123\n",
      "Epoch 23018: 8429.1543 \t time  130\n",
      "Epoch 23019: 8489.9355 \t time  137\n",
      "Epoch 23020: 8522.6553 \t time  143\n",
      "Epoch 23021: 8498.9268 \t time  150\n",
      "Epoch 23022: 8527.1602 \t time  157\n",
      "Epoch 23023: 8462.2031 \t time  163\n",
      "Epoch 23024: 8520.7246 \t time  170\n",
      "Epoch 23025: 8475.8721 \t time  177\n",
      "Epoch 23026: 8473.6738 \t time  184\n",
      "Epoch 23027: 8540.0605 \t time  191\n",
      "Epoch 23028: 8572.4580 \t time  198\n",
      "Epoch 23029: 8513.5771 \t time  204\n",
      "Epoch 23030: 8469.4434 \t time  211\n",
      "Epoch 23031: 8517.8164 \t time  218\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random as ra\n",
    "import os\n",
    "import glob\n",
    "from numpy.random import *\n",
    "import time\n",
    "\n",
    "# 計算グラフ\n",
    "g2 = tf.Graph()\n",
    "with g2.as_default():\n",
    "    # プレースホルダー\n",
    "    x_ = tf.placeholder(tf.float32, shape=(None, 128, 128, 3))\n",
    "    y_ = tf.placeholder(tf.float32, shape=(None, 100))\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    # 畳み込み層1\n",
    "    with tf.name_scope('conv1'):\n",
    "        conv1_features = 40 # 畳み込み層1の出力次元数\n",
    "        max_pool_size1 = 2 # 畳み込み層1のマックスプーリングサイズ\n",
    "        conv1_w = tf.Variable(tf.truncated_normal([5, 5, 3, conv1_features], stddev=0.1), dtype=tf.float32, name='conv1_w') # 畳み込み層1の重み(初期値)\n",
    "        conv1_b = tf.Variable(tf.constant(0.1, shape=[conv1_features]), dtype=tf.float32, name='conv1_b') # 畳み込み層1のバイアス(初期値)\n",
    "        conv1_c2 = tf.nn.conv2d(x_, conv1_w, strides=[1, 1, 1, 1], padding=\"SAME\", name='conv1_conv2d') # 畳み込み層1-畳み込み\n",
    "        conv1_relu = tf.nn.relu(conv1_c2+conv1_b, name='conv1_ReLU') #畳み込み層1-ReLU\n",
    "        conv1_drop = tf.nn.dropout(conv1_relu, keep_prob, name='conv1_dropout')#畳み込み層1-ドロップアウト\n",
    "        conv1_mp = tf.nn.max_pool(conv1_drop, ksize=[1, max_pool_size1, max_pool_size1, 1], strides=[1, max_pool_size1, max_pool_size1, 1], padding=\"SAME\", name='conv1_max_polling') #畳み込み層1-マックスプーリング\n",
    "\n",
    "    # 畳み込み層2\n",
    "    with tf.name_scope('conv2'):\n",
    "        conv2_features = 80 # 畳み込み層2の出力次元数\n",
    "        max_pool_size2 = 2 # 畳み込み層2のマックスプーリングのサイズ\n",
    "        conv2_w = tf.Variable(tf.truncated_normal([5, 5, conv1_features, conv2_features], stddev=0.1), dtype=tf.float32, name='conv2_w') # 畳み込み層2の重み\n",
    "        conv2_b = tf.Variable(tf.constant(0.1, shape=[conv2_features]), dtype=tf.float32, name='conv2_b') # 畳み込み層2のバイアス\n",
    "        conv2_c2 = tf.nn.conv2d(conv1_mp, conv2_w, strides=[1, 1, 1, 1], padding=\"SAME\", name='conv2_conv2d') # 畳み込み層2-畳み込み\n",
    "        conv2_relu = tf.nn.relu(conv2_c2+conv2_b, name='conv2_ReLU') # 畳み込み層2-ReLU\n",
    "        conv2_drop = tf.nn.dropout(conv2_relu, keep_prob, name='conv2_dropout')#畳み込み層2-ドロップアウト\n",
    "        conv2_mp = tf.nn.max_pool(conv2_drop, ksize=[1, max_pool_size2, max_pool_size2, 1], strides=[1, max_pool_size2, max_pool_size2, 1], padding=\"SAME\", name='conv2_max_polling') # 畳み込み層2-マックスプーリング\n",
    "\n",
    "    # 畳み込み層3\n",
    "    with tf.name_scope('conv3'):\n",
    "        conv3_features = 160 # 畳み込み層3の出力次元数\n",
    "        max_pool_size3 = 2 # 畳み込み層3のマックスプーリングのサイズ\n",
    "        conv3_w = tf.Variable(tf.truncated_normal([5, 5, conv2_features, conv3_features], stddev=0.1), dtype=tf.float32, name='conv3_w') # 畳み込み層3の重み\n",
    "        conv3_b = tf.Variable(tf.constant(0.1, shape=[conv3_features]), dtype=tf.float32, name='conv3_b') # 畳み込み層3のバイアス\n",
    "        conv3_c2 = tf.nn.conv2d(conv2_mp, conv3_w, strides=[1, 1, 1, 1], padding=\"SAME\", name='conv3_conv2d') # 畳み込み層3-畳み込み\n",
    "        conv3_relu = tf.nn.relu(conv3_c2+conv3_b, name='conv3_ReLU') # 畳み込み層3-ReLU\n",
    "        conv3_drop = tf.nn.dropout(conv3_relu, keep_prob, name='conv3_dropout')#畳み込み層3-ドロップアウト\n",
    "        conv3_mp = tf.nn.max_pool(conv3_drop, ksize=[1, max_pool_size3, max_pool_size3, 1], strides=[1, max_pool_size3, max_pool_size3, 1], padding=\"SAME\", name='conv3_max_polling') # 畳み込み層3-マックスプーリング\n",
    "    \n",
    "    # 畳み込み層4\n",
    "    with tf.name_scope('conv4'):\n",
    "        conv4_features = 320 # 畳み込み層4の出力次元数\n",
    "        max_pool_size4 = 2 # 畳み込み層4のマックスプーリングのサイズ\n",
    "        conv4_w = tf.Variable(tf.truncated_normal([5, 5, conv3_features, conv4_features], stddev=0.1), dtype=tf.float32, name='conv4_w') # 畳み込み層4の重み\n",
    "        conv4_b = tf.Variable(tf.constant(0.1, shape=[conv4_features]), dtype=tf.float32, name='conv4_b') # 畳み込み層4のバイアス\n",
    "        conv4_c2 = tf.nn.conv2d(conv3_mp, conv4_w, strides=[1, 1, 1, 1], padding=\"SAME\", name='conv4_conv2d') # 畳み込み層4-畳み込み\n",
    "        conv4_relu = tf.nn.relu(conv4_c2+conv4_b, name='conv4_ReLU') # 畳み込み層4-ReLU\n",
    "        conv4_drop = tf.nn.dropout(conv4_relu, keep_prob, name='conv4_dropout')#畳み込み層4-ドロップアウト\n",
    "        conv4_mp = tf.nn.max_pool(conv4_drop, ksize=[1, max_pool_size4, max_pool_size4, 1], strides=[1, max_pool_size4, max_pool_size4, 1], padding=\"SAME\", name='conv4_max_polling') # 畳み込み層4-マックスプーリング\n",
    "\n",
    "    # 畳み込み層5\n",
    "    with tf.name_scope('conv5'):\n",
    "        conv5_features = 640 # 畳み込み層5の出力次元数\n",
    "        max_pool_size5 = 2 # 畳み込み層5のマックスプーリングのサイズ\n",
    "        conv5_w = tf.Variable(tf.truncated_normal([5, 5, conv4_features, conv5_features], stddev=0.1), dtype=tf.float32, name='conv5_w') # 畳み込み層5の重み\n",
    "        conv5_b = tf.Variable(tf.constant(0.1, shape=[conv5_features]), dtype=tf.float32, name='conv5_b') # 畳み込み層5のバイアス\n",
    "        conv5_c2 = tf.nn.conv2d(conv4_mp, conv5_w, strides=[1, 1, 1, 1], padding=\"SAME\", name='conv5_conv2d') # 畳み込み層5-畳み込み\n",
    "        conv5_relu = tf.nn.relu(conv5_c2+conv5_b, name='conv5_ReLU') # 畳み込み層5-ReLU\n",
    "        conv5_drop = tf.nn.dropout(conv5_relu, keep_prob, name='conv5_dropout')#畳み込み層5-ドロップアウト\n",
    "        conv5_mp = tf.nn.max_pool(conv5_drop, ksize=[1, max_pool_size5, max_pool_size5, 1], strides=[1, max_pool_size5, max_pool_size5, 1], padding=\"SAME\", name='conv5_max_polling') # 畳み込み層5-マックスプーリング\n",
    "\n",
    "    # 畳み込み層6\n",
    "    with tf.name_scope('conv6'):\n",
    "        conv6_features = 1280 # 畳み込み層6の出力次元数\n",
    "        max_pool_size6 = 2 # 畳み込み層6のマックスプーリングのサイズ\n",
    "        conv6_w = tf.Variable(tf.truncated_normal([5, 5, conv5_features, conv6_features], stddev=0.1), dtype=tf.float32, name='conv6_w') # 畳み込み層6の重み\n",
    "        conv6_b = tf.Variable(tf.constant(0.1, shape=[conv6_features]), dtype=tf.float32, name='conv6_b') # 畳み込み層6のバイアス\n",
    "        conv6_c2 = tf.nn.conv2d(conv5_mp, conv6_w, strides=[1, 1, 1, 1], padding=\"SAME\", name='conv6_conv2d') # 畳み込み層6-畳み込み\n",
    "        conv6_relu = tf.nn.relu(conv6_c2+conv6_b, name='conv6_ReLU') # 畳み込み層6-ReLU\n",
    "        conv6_drop = tf.nn.dropout(conv6_relu, keep_prob, name='conv6_dropout')#畳み込み層6-ドロップアウト\n",
    "        conv6_mp = tf.nn.max_pool(conv6_drop, ksize=[1, max_pool_size6, max_pool_size6, 1], strides=[1, max_pool_size6, max_pool_size6, 1], padding=\"SAME\", name='conv6_max_polling') # 畳み込み層6-マックスプーリング\n",
    "        \n",
    "    \n",
    "    # 全結合層1\n",
    "    with tf.name_scope('fully1'):\n",
    "        result_w = x_.shape[1] // (max_pool_size1*max_pool_size2*max_pool_size3*max_pool_size4*max_pool_size5*max_pool_size6)\n",
    "        result_h = x_.shape[2] // (max_pool_size1*max_pool_size2*max_pool_size3*max_pool_size4*max_pool_size5*max_pool_size6)\n",
    "        fc_input_size = result_w * result_h * conv6_features # 畳み込んだ結果、全結合層に入力する次元数\n",
    "        fc_features = 1000 # 全結合層1の出力次元数（隠れ層の次元数）\n",
    "        s = conv6_mp.get_shape().as_list() # [None, result_w, result_h, conv4_features]\n",
    "        conv_result = tf.reshape(conv6_mp, [-1, s[1]*s[2]*s[3]]) # 畳み込みの結果を1*N層に変換\n",
    "        fc1_w = tf.Variable(tf.truncated_normal([fc_input_size.value, fc_features], stddev=0.1), dtype=tf.float32, name='fully1_w') # 重み\n",
    "        fc1_b = tf.Variable(tf.constant(0.1, shape=[fc_features]), dtype=tf.float32, name='fully1_b') # バイアス\n",
    "        fc1 = tf.nn.relu(tf.matmul(conv_result, fc1_w)+fc1_b, name='fully1_ReLU') # 全結合層1\n",
    "        fc1_drop = tf.nn.dropout(fc1, keep_prob, name='fully1_dropout') #ドロップアウト\n",
    "    \n",
    "    # 全結合層2\n",
    "    with tf.name_scope('fully2'):\n",
    "        fc2_features = 1000 # 全結合層2の出力次元数（隠れ層の次元数）\n",
    "        fc2_w = tf.Variable(tf.truncated_normal([fc_features, fc2_features], stddev=0.1), dtype=tf.float32, name='fully2_w') # 重み\n",
    "        fc2_b = tf.Variable(tf.constant(0.1, shape=[fc2_features]), dtype=tf.float32, name='fully2_b') # バイアス\n",
    "        fc2 = tf.nn.relu(tf.matmul(fc1_drop, fc2_w)+fc2_b, name='fully2_ReLU') # 全結合層2\n",
    "        fc2_drop = tf.nn.dropout(fc2, keep_prob, name='fully2_dropout')#ドロップアウト\n",
    "\n",
    "    # 全結合層3\n",
    "    with tf.name_scope('fully3'):\n",
    "        fc3_features = 500 # 全結合層3の出力次元数（隠れ層の次元数）\n",
    "        fc3_w = tf.Variable(tf.truncated_normal([fc2_features, fc3_features], stddev=0.1), dtype=tf.float32, name='fully3_w') # 重み\n",
    "        fc3_b = tf.Variable(tf.constant(0.1, shape=[fc3_features]), dtype=tf.float32, name='fully3_b') # バイアス\n",
    "        fc3 = tf.nn.relu(tf.matmul(fc2_drop, fc3_w)+fc3_b, name='fully3_ReLU') # 全結合層3\n",
    "        fc3_drop = tf.nn.dropout(fc3, keep_prob, name='fully3_dropout')#ドロップアウト\n",
    "    \n",
    "    # 全結合層4\n",
    "    with tf.name_scope('fully4'):\n",
    "        fc4_features = 100 # 全結合層3の出力次元数（隠れ層の次元数）\n",
    "        fc4_w = tf.Variable(tf.truncated_normal([fc3_features, fc4_features], stddev=0.1), dtype=tf.float32, name='fully4_w') # 重み\n",
    "        fc4_b = tf.Variable(tf.constant(0.1, shape=[fc4_features]), dtype=tf.float32, name='fully4_b') # バイアス\n",
    "        y = tf.matmul(fc3_drop, fc4_w)+fc4_b\n",
    "\n",
    "    # 平均2乗和誤差(square_error)\n",
    "    with tf.name_scope('loss'):\n",
    "        loss = tf.reduce_sum(tf.square(y_ - y), name='loss')\n",
    "    \n",
    "    # 勾配法\n",
    "    with tf.name_scope('train'):\n",
    "        train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "    # グラフを保存する．\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "#実行フェーズ\n",
    "start = time.time()\n",
    "\n",
    "n_epochs = 50000000\n",
    "training_loss = []\n",
    "eps = 0.01\n",
    "batch_size = 60\n",
    "batch_x, batch_z = batch(batch_size)\n",
    "kp_p = 1.0\n",
    "\n",
    "epo = 23000\n",
    "\n",
    "with tf.Session(graph=g2) as sess:\n",
    "    #実行フェーズでの値の復元\n",
    "    saver.restore(sess, './savepoint/inverter_graph/train_epoch_'+str(epo)+'/trained-model')\n",
    "    \n",
    "    \"\"\"loss = sess.run('loss/loss:0', feed_dict={x_ : batch_x, y_ : batch_z, keep_prob : kp_p})\n",
    "    train_step = sess.run('train/Adam', feed_dict={x_ : batch_x, y_ : batch_z, keep_prob : kp_p})\n",
    "    print(loss)\n",
    "    print(train_step)\"\"\"\n",
    "    \n",
    "    # TensorBoardで追跡する変数を定義\n",
    "    with tf.name_scope('summary'):\n",
    "        tf.summary.scalar('loss', loss)\n",
    "        merged = tf.summary.merge_all()\n",
    "        writer = tf.summary.FileWriter('logsdata/inverter_graph_try', sess.graph)\n",
    "    \n",
    "    #500エポックでモデルをトレーニング\n",
    "    for e in range(n_epochs):\n",
    "        c, _, summary = sess.run([loss, train_step, merged], feed_dict={x_: batch_x, y_: batch_z, keep_prob: 0.5})\n",
    "        writer.add_summary(summary, e)\n",
    "        training_loss.append(c)\n",
    "        \n",
    "        e = epo+e+1\n",
    "        print('Epoch %4d: %.4f' % (e, c), '\\t', 'time %4d' %(time.time()-start))\n",
    "        \n",
    "        if not e % 50:\n",
    "            batch_x, batch_z = batch(batch_size)\n",
    "        \n",
    "        if(e > 0 and c < eps):\n",
    "            saver.save(sess, './savepoint/inverter_graph_try/train_epoch_'+str(e)+'/trained-model')\n",
    "            break\n",
    "        \n",
    "        if(e % 1000 == 0):\n",
    "            saver.save(sess, './savepoint/inverter_graph_try/train_epoch_'+str(e)+'/trained-model')\n",
    "        e = e-epo-1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
