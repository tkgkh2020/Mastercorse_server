{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib import cm\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import fetch_mldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ネットワークの定義\n",
    " \n",
    "# プレースホルダー\n",
    "x_ = tf.placeholder(tf.float32, shape=(None, 28, 28, 1))\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 畳み込み層1\n",
    "conv1_features = 20 # 畳み込み層1の出力次元数\n",
    "max_pool_size1 = 2 # 畳み込み層1のマックスプーリングサイズ\n",
    "conv1_w = tf.Variable(tf.truncated_normal([5, 5, 1, conv1_features], stddev=0.1), dtype=tf.float32) # 畳み込み層1の重み\n",
    "conv1_b = tf.Variable(tf.constant(0.1, shape=[conv1_features]), dtype=tf.float32) # 畳み込み層1のバイアス\n",
    "conv1_c2 = tf.nn.conv2d(x_, conv1_w, strides=[1, 1, 1, 1], padding=\"SAME\") # 畳み込み層1-畳み込み\n",
    "conv1_relu = tf.nn.relu(conv1_c2+conv1_b) # 畳み込み層1-ReLU\n",
    "conv1_mp = tf.nn.max_pool(conv1_relu, ksize=[1, max_pool_size1, max_pool_size1, 1], \n",
    "                          strides=[1, max_pool_size1, max_pool_size1, 1], padding=\"SAME\") # 畳み込み層1-マックスプーリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 畳み込み層2\n",
    "conv2_features = 50 # 畳み込み層2の出力次元数\n",
    "max_pool_size2 = 2 # 畳み込み層2のマックスプーリングのサイズ\n",
    "conv2_w = tf.Variable(tf.truncated_normal([5, 5, conv1_features, conv2_features], stddev=0.1), dtype=tf.float32) # 畳み込み層2の重み\n",
    "conv2_b = tf.Variable(tf.constant(0.1, shape=[conv2_features]), dtype=tf.float32) # 畳み込み層2のバイアス\n",
    "conv2_c2 = tf.nn.conv2d(conv1_mp, conv2_w, strides=[1, 1, 1, 1], padding=\"SAME\") # 畳み込み層2-畳み込み\n",
    "conv2_relu = tf.nn.relu(conv2_c2+conv2_b) # 畳み込み層2-ReLU\n",
    "conv2_mp = tf.nn.max_pool(conv2_relu, ksize=[1, max_pool_size2, max_pool_size2, 1],\n",
    "                          strides=[1, max_pool_size2, max_pool_size2, 1], padding=\"SAME\") # 畳み込み層2-マックスプーリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全結合層1\n",
    "result_w = x_.shape[1] // (max_pool_size1*max_pool_size2)\n",
    "result_h = x_.shape[2] // (max_pool_size1*max_pool_size2)\n",
    "fc_input_size = result_w * result_h * conv2_features # 畳み込んだ結果、全結合層に入力する次元数\n",
    "fc_features = 500 # 全結合層の出力次元数（隠れ層の次元数）\n",
    "s = conv2_mp.get_shape().as_list() # [None, result_w, result_h, conv2_features]\n",
    "conv_result = tf.reshape(conv2_mp, [-1, s[1]*s[2]*s[3]]) # 畳み込みの結果を1*N層に変換\n",
    "fc1_w = tf.Variable(tf.truncated_normal([fc_input_size.value, fc_features], stddev=0.1), dtype=tf.float32) # 重み\n",
    "fc1_b = tf.Variable(tf.constant(0.1, shape=[fc_features]), dtype=tf.float32) # バイアス\n",
    "fc1 = tf.nn.relu(tf.matmul(conv_result, fc1_w)+fc1_b) # 全結合層1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全結合層2\n",
    "fc2_w = tf.Variable(tf.truncated_normal([fc_features, fc_features], stddev=0.1), dtype=tf.float32) # 重み\n",
    "fc2_b = tf.Variable(tf.constant(0.1, shape=[fc_features]), dtype=tf.float32) # バイアス\n",
    "fc2 = tf.nn.relu(tf.matmul(fc1, fc2_w)+fc2_b) # 全結合層2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全結合層3\n",
    "fc3_w = tf.Variable(tf.truncated_normal([fc_features, 10], stddev=0.1), dtype=tf.float32) # 重み\n",
    "fc3_b = tf.Variable(tf.constant(0.1, shape=[10]), dtype=tf.float32) # バイアス\n",
    "y = tf.matmul(fc2, fc3_w)+fc3_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# クロスエントロピー誤差\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "# 勾配法\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "# 正解率の計算\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習\n",
    " \n",
    "EPOCH_NUM = 5\n",
    "BATCH_SIZE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 教師データ\n",
    "mnist = fetch_mldata('MNIST original', data_home='.')\n",
    "mnist.data = mnist.data.astype(np.float32) # 画像データ　784*70000 [[0-255, 0-255, ...], [0-255, 0-255, ...], ... ]\n",
    "mnist.data /= 255 # 0-1に正規化する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist.target = mnist.target.astype(np.int32) # ラベルデータ70000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 教師データを変換\n",
    "N = 60000\n",
    "train_x, test_x = np.split(mnist.data,   [N]) # 教師データ\n",
    "train_y, test_y = np.split(mnist.target, [N]) # テスト用のデータ\n",
    "train_x = train_x.reshape((len(train_x), 28, 28, 1)) # (N, height, width, channel)\n",
    "test_x = test_x.reshape((len(test_x), 28, 28, 1))\n",
    "# ラベルはone-hotベクトルに変換する\n",
    "train_y = np.eye(np.max(train_y)+1)[train_y]\n",
    "test_y = np.eye(np.max(test_y)+1)[test_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習\n",
    "print(\"Train\")\n",
    "with tf.Session() as sess:\n",
    "    st = time.time()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(EPOCH_NUM):\n",
    "        perm = np.random.permutation(N)\n",
    "        total_loss = 0\n",
    "        for i in range(0, N, BATCH_SIZE):\n",
    "            batch_x = train_x[perm[i:i+BATCH_SIZE]]\n",
    "            batch_y = train_y[perm[i:i+BATCH_SIZE]]\n",
    "            total_loss += cross_entropy.eval(feed_dict={x_: batch_x, y_: batch_y})\n",
    "            train_step.run(feed_dict={x_: batch_x, y_: batch_y})\n",
    "        test_accuracy = accuracy.eval(feed_dict={x_: test_x, y_: test_y})\n",
    "        if (epoch+1) % 1 == 0:\n",
    "            ed = time.time()\n",
    "            print(\"epoch:\\t{}\\ttotal loss:\\t{}\\tvaridation accuracy:\\t{}\\ttime:\\t{}\".format(epoch+1, total_loss, test_accuracy, ed-st))\n",
    "            st = time.time()\n",
    " \n",
    "    # 予測\n",
    "    print(\"\\nPredict\")\n",
    "    idx = np.random.choice(70000-N, 10)\n",
    "    for i in idx:\n",
    "        pre = np.argmax(y.eval(feed_dict={x_: [test_x[i]]}))\n",
    "        plt.figure(figsize=(1,1))\n",
    "        plt.imshow(test_x[i].reshape(28,28), cmap=cm.gray_r)\n",
    "        plt.show()\n",
    "        print(\"=&gt;\", pre, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
